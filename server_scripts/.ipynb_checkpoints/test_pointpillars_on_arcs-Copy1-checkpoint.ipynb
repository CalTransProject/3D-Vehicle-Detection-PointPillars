{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa9501ae",
   "metadata": {},
   "source": [
    "# Test PointPillars on ARCS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f9487a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import bbox\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from contextlib import redirect_stdout\n",
    "from mmdet3d.apis import LidarDet3DInferencer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "df4d9c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection3d/v1.0.0_models/pointpillars/hv_pointpillars_secfpn_6x8_160e_kitti-3d-3class/hv_pointpillars_secfpn_6x8_160e_kitti-3d-3class_20220301_150306-37dc2420.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rachel/mmdetection3d/mmdet3d/models/dense_heads/anchor3d_head.py:94: UserWarning: dir_offset and dir_limit_offset will be depressed and be incorporated into box coder in the future\n",
      "  warnings.warn(\n",
      "/home/rachel/miniconda3/envs/openmm_mmvc/lib/python3.8/site-packages/mmengine/visualization/visualizer.py:196: UserWarning: Failed to add <class 'mmengine.visualization.vis_backend.LocalVisBackend'>, please provide the `save_dir` argument.\n",
      "  warnings.warn(f'Failed to add {vis_backend.__class__}, '\n"
     ]
    }
   ],
   "source": [
    "# Initialize inferencer\n",
    "inferencer = LidarDet3DInferencer('pointpillars_kitti-3class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c83c863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory paths\n",
    "ARCS_LABELS = '../data/eval_data/labels'\n",
    "ARCS_DATA_DIR = '../data/eval_data/arcs'\n",
    "ARCS_FILTERED_DATA_DIR = '../data/eval_data/arcs_azimuth_filtered'\n",
    "ARCS_LABEL_FILTERED_DATA_DIR = '../data/eval_data/arcs_label_filtered'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d48103d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3D bbox object requires quaternion values. This function converts the yaw to these values\n",
    "def yaw_to_quaternion(yaw):\n",
    "    \"\"\"\n",
    "    Converts a yaw angle (rotation about the z-axis) to quaternion coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    - yaw (float): The yaw angle in radians.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple of (rw, rx, ry, rz): The quaternion representation of the yaw.\n",
    "    \"\"\"\n",
    "    # Compute the components of the quaternion\n",
    "    rw = math.cos(yaw / 2.0)\n",
    "    rz = math.sin(yaw / 2.0)\n",
    "    \n",
    "    # rx and ry are zero because the rotation is only about the z-axis\n",
    "    return (rw, 0, 0, rz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "127142a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes x, y, z, dx, dy, dz, yaw and returns a 3D bbox object from the bbox package\n",
    "# Yaw is in radians\n",
    "def get3DBbox(x, y, z, dx, dy, dz, yaw):\n",
    "    # Example usage:\n",
    "    rw, rx, ry, rz = yaw_to_quaternion(yaw)\n",
    "    bbox_obj = bbox.BBox3D(x, y, z, length=dx, width=dy, height=dz, rw=rw, rx=rx, ry=ry, rz=rz)\n",
    "    return bbox_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c64892fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_iou(bbox1, bbox2):\n",
    "    bbox_obj1 = get3DBbox(*bbox1)\n",
    "    bbox_obj2 = get3DBbox(*bbox2)\n",
    "    return bbox.metrics.jaccard_index_3d(bbox_obj1, bbox_obj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8ab78e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _evaluate frame takes a prediction dictionary output by the model, and a list of ground truths\n",
    "# from the label file, and returns the TPs, FPs, and FNs for one lidar frame\n",
    "def _evaluate_frame(predictions, ground_truths, threshold=0.25):\n",
    "    TPs, FPs, FNs = 0, 0, len(ground_truths)\n",
    "    used_gt = set()\n",
    "    \n",
    "    for pred in predictions['predictions']:\n",
    "        for label, score, bbox in zip(pred['labels_3d'], pred['scores_3d'], pred['bboxes_3d']):\n",
    "            if score < threshold:\n",
    "                continue\n",
    "\n",
    "            best_iou = 0\n",
    "            best_gt = None\n",
    "            for gt in ground_truths:\n",
    "                iou = calculate_iou(bbox, gt[8:])\n",
    "                if iou > best_iou:\n",
    "                    best_iou = iou\n",
    "                    best_gt = tuple(gt)\n",
    "\n",
    "            if best_iou > threshold:  # Example IoU threshold for a match\n",
    "                if best_gt not in used_gt:\n",
    "                    used_gt.add(best_gt)\n",
    "                    TPs += 1\n",
    "                    FNs -= 1\n",
    "                else:\n",
    "                    FPs += 1\n",
    "            else:\n",
    "                FPs += 1\n",
    "    return TPs, FPs, FNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17acf892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ground_truths(label_path):\n",
    "    labels = []\n",
    "    with open(label_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            bbox = []\n",
    "            # Add the category as a string\n",
    "            bbox.append(parts[0])\n",
    "            # Extract the bounding box dimensions and location as \n",
    "            bbox = bbox + [float(value) for value in parts[1:15]]  \n",
    "            bbox[2] = int(bbox[2])\n",
    "            labels.append(bbox)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5d2c64fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_problem_file_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2474084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _evaluate frame takes a lidar and label file path and\n",
    "# and returns the TPs, FPs, and FNs for one lidar frame\n",
    "def evaluate_frame(lidar_file_path, label_file_path):\n",
    "    inputs = dict(points=str(lidar_file_path))\n",
    "    \n",
    "    # Get predictions\n",
    "    try:\n",
    "        predictions = inferencer(inputs)\n",
    "            # Get ground truths\n",
    "        ground_truths = parse_ground_truths(label_file_path)\n",
    "\n",
    "        TPs, FPs, FNs = _evaluate_frame(predictions, ground_truths)\n",
    "    except:\n",
    "        # Add file id to the problem file list\n",
    "        print(str(lidar_file_path))\n",
    "        lidar_filename = os.path.basename(lidar_file_path)\n",
    "        file_id, extension = os.path.splitext(lidar_filename)\n",
    "        generated_problem_file_list.append(file_id)\n",
    "        TPs, FPs, FNs = 0, 0, 0\n",
    "    \n",
    "    return TPs, FPs, FNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e35c8de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem files. I'll find out why later\n",
    "problem_file_ids = ['000522', '003066', '000556', '004340', '001016', '004224', '005318', '004344', '001014', '001027', \n",
    "                    '006440', '001026', '000869', '001031', '000530', '004928', '000765', '004926', '003395', '000864', \n",
    "                    '004226', '000550', '006437', '000538', '004930', '000527', '004223', '000766', '001012', '005320', \n",
    "                    '006438', '004014', '001010', '001013', '003439', '004016', '001008', '000866', '000662', '001015', \n",
    "                    '000523', '001018', '000554', '000524', '000528', '006445', '005156', '004347', '000539', '000533', \n",
    "                    '004225', '004013', '004227', '000558', '003436', '004348', '000525', '004345', '004015', '005319', \n",
    "                    '004012', '005321', '003397', '000541', '000534', '006436', '000549', '000553', '000767', '001023', \n",
    "                    '006442', '000865', '003065', '004346', '003398', '004230', '000764', '000665', '000520', '003067', \n",
    "                    '006444', '001011', '000763', '005157', '001025', '000548', '005158', '000543', '001019', '001024', \n",
    "                    '001033', '005155', '000868', '004288', '000542', '003440', '000529', '006443', '000762', '000535', \n",
    "                    '004342', '000540', '001032', '000552', '004011', '004251', '004929', '004054', '001021', '000545', \n",
    "                    '004228', '000443', '001030', '000444', '000544', '001022', '001034', '006439', '001028', '003437', \n",
    "                    '004252', '006441', '003399', '005317', '001017', '000863', '003144', '000663', '001009', '005322', \n",
    "                    '000526', '000532', '003064', '000768', '005154', '003063', '004289', '000557', '004250', '000551', \n",
    "                    '004253', '004927', '000536', '000521', '000445', '004229', '000555', '000867', '000446', '004287', \n",
    "                    '004291', '000546', '004290', '000537', '003396', '004343', '000531', '001029', '000664', '003438', \n",
    "                    '001020', '000547', '000559', '004341']\n",
    "okay_file_ids = ['001731', '005977', '005368', '005924', '004402']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6bb44482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(list_file_ids):\n",
    "    for id in list_file_ids:\n",
    "        print(f'\\n_____________________________________________________________')\n",
    "        \n",
    "        file_path = f'{ARCS_LABEL_FILTERED_DATA_DIR}/{id}.bin'\n",
    "        label_path = f'{ARCS_LABELS}/{id}.txt' \n",
    "\n",
    "        # Open point cloud file\n",
    "        points = np.fromfile(file_path, dtype=np.float32).reshape(-1, 4)\n",
    "        df_points = pd.DataFrame(points, columns=['x', 'y', 'z', 'intensity'])\n",
    "\n",
    "        # Print the DataFrame summary\n",
    "        print(f\"Summary for file ID {id}:\")\n",
    "        print('length: ' + str(len(df_points)))\n",
    "        print(\"DataFrame Info:\")\n",
    "        df_points.info()\n",
    "        print(\"\\nDataFrame Description:\")\n",
    "        print(df_points.describe())\n",
    "        print(f'\\nlabels:')\n",
    "        # Print labels\n",
    "        with open(label_path, 'r') as file:\n",
    "            for line in file:\n",
    "                print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b3582882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_summary(problem_file_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "465bc5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print_summary(okay_file_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "bab7d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(dataset_path):\n",
    "    label_dir = Path(ARCS_LABELS)\n",
    "    dataset_dir = Path(dataset_path)\n",
    "    \n",
    "    total_TPs, total_FPs, total_FNs = 0, 0, 0\n",
    "\n",
    "    for bin_file in itertools.islice(dataset_dir.iterdir(), 10):\n",
    "#     for bin_file in dataset_dir.iterdir():\n",
    "        if str(bin_file).endswith('.bin'):\n",
    "            print('.', end='')\n",
    "\n",
    "            lidar_filename = os.path.basename(bin_file)\n",
    "            # Split the filename from the extension ('006428', '.txt')\n",
    "            file_id, extension = os.path.splitext(lidar_filename)\n",
    "            if file_id not in problem_file_ids:\n",
    "                label_filename = file_id + '.txt'\n",
    "\n",
    "                # Make file paths\n",
    "                lidar_file_path = Path(dataset_dir, lidar_filename)\n",
    "                label_file_path = Path(label_dir, label_filename)\n",
    "                \n",
    "                print('evaluating: ' + str(lidar_file_path))\n",
    "                \n",
    "                TPs, FPs, FNs = evaluate_frame(lidar_file_path, label_file_path)\n",
    "\n",
    "                total_TPs += TPs\n",
    "                total_FPs += FPs\n",
    "                total_FNs += FNs\n",
    "            else:\n",
    "                print('skipping ' + file_id)\n",
    "            \n",
    "    return total_TPs, total_FPs, total_FNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ea5180b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pointpillars(dataset_path, dataset_name):\n",
    "    print('Evaluating dataset: ' + dataset_path)\n",
    "    # Get all TP, FP, and FN in the dataset\n",
    "    TPs, FPs, FNs = evaluate_dataset(dataset_path)\n",
    "    # Get metrics\n",
    "    precision = TPs / (TPs + FPs) if TPs + FPs > 0 else 0\n",
    "    recall = TPs / (TPs + FNs) if TPs + FNs > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Placeholder for average precision calculation\n",
    "    average_precision = 0  # This will need actual precision-recall curve data\n",
    "\n",
    "    # Organize results into a dictionary\n",
    "    results = {\n",
    "        'Dataset': dataset_name,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1_score,\n",
    "        'Average Precision': average_precision\n",
    "    }\n",
    "    print(results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "47f30b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run tests on ARCS\n",
    "results = test_pointpillars(ARCS_DATA_DIR, 'ARCS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cdb53c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Metrics for ARCS')\n",
    "# print('Precision: ' + str(precision))\n",
    "# print('Recall: ' + str(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b427d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run tests on ARCS filtered\n",
    "filter_results = test_pointpillars(ARCS_FILTERED_DATA_DIR, 'Filtered ARCS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e5e24baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Metrics for Filtered ARCS')\n",
    "# print('Precision: ' + str(filter_precision))\n",
    "# print('Recall: ' + str(filter_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4159272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run tests on ARCS label filtered\n",
    "label_filter_results = test_pointpillars(ARCS_LABEL_FILTERED_DATA_DIR, 'Label Filtered ARCS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "09396d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Metrics for Label Filtered ARCS')\n",
    "# print('Precision: ' + str(label_filter_precision))\n",
    "# print('Recall: ' + str(label_filter_recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "b703498d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(generated_problem_file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f030e4b0",
   "metadata": {},
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cb79b3af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dataset': 'ARCS', 'Precision': 0.2702702702702703, 'Recall': 0.7407407407407407, 'F1 Score': 0.396039603960396, 'Average Precision': 0}\n",
      "{'Dataset': 'Filtered ARCS', 'Precision': 0.7692307692307693, 'Recall': 0.37037037037037035, 'F1 Score': 0.5, 'Average Precision': 0}\n",
      "{'Dataset': 'Label Filtered ARCS', 'Precision': 0.95, 'Recall': 0.7037037037037037, 'F1 Score': 0.8085106382978724, 'Average Precision': 0}\n"
     ]
    }
   ],
   "source": [
    "print(results)\n",
    "print(filter_results)\n",
    "print(label_filter_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a0dbad4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Average Precision</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARCS</td>\n",
       "      <td>0.270270</td>\n",
       "      <td>0.740741</td>\n",
       "      <td>0.396040</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Filtered ARCS</td>\n",
       "      <td>0.769231</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Label Filtered ARCS</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.703704</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Dataset  Precision    Recall  F1 Score  Average Precision\n",
       "0                 ARCS   0.270270  0.740741  0.396040                  0\n",
       "1        Filtered ARCS   0.769231  0.370370  0.500000                  0\n",
       "2  Label Filtered ARCS   0.950000  0.703704  0.808511                  0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "results_df = pd.DataFrame([results, filter_results, label_filter_results])\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "572182fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Dataset  Precision   Recall  F1 Score  Average Precision\n",
      "               ARCS   0.270270 0.740741  0.396040                  0\n",
      "      Filtered ARCS   0.769231 0.370370  0.500000                  0\n",
      "Label Filtered ARCS   0.950000 0.703704  0.808511                  0\n"
     ]
    }
   ],
   "source": [
    "print(results_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
