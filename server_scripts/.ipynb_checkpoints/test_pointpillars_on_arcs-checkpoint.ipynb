{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa9501ae",
   "metadata": {},
   "source": [
    "# Test PointPillars on ARCS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "f9487a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import bbox\n",
    "import itertools\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from contextlib import redirect_stdout\n",
    "from mmdet3d.apis import LidarDet3DInferencer\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "df4d9c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loads checkpoint by http backend from path: https://download.openmmlab.com/mmdetection3d/v1.0.0_models/pointpillars/hv_pointpillars_secfpn_6x8_160e_kitti-3d-3class/hv_pointpillars_secfpn_6x8_160e_kitti-3d-3class_20220301_150306-37dc2420.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rachel/mmdetection3d/mmdet3d/models/dense_heads/anchor3d_head.py:94: UserWarning: dir_offset and dir_limit_offset will be depressed and be incorporated into box coder in the future\n",
      "  warnings.warn(\n",
      "/home/rachel/miniconda3/envs/openmm_mmvc/lib/python3.8/site-packages/mmengine/visualization/visualizer.py:196: UserWarning: Failed to add <class 'mmengine.visualization.vis_backend.LocalVisBackend'>, please provide the `save_dir` argument.\n",
      "  warnings.warn(f'Failed to add {vis_backend.__class__}, '\n"
     ]
    }
   ],
   "source": [
    "# Initialize inferencer\n",
    "inferencer = LidarDet3DInferencer('pointpillars_kitti-3class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c83c863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory paths\n",
    "ARCS_LABELS = '../data/eval_data/labels'\n",
    "ARCS_DATA_DIR = '../data/eval_data/arcs'\n",
    "ARCS_LOW_DEF_FILTERED_DATA_DIR = '../data/eval_data/arcs_low_def_filtered'\n",
    "ARCS_AZIMUTH_FILTERED_DATA_DIR = '../data/eval_data/arcs_azimuth_filtered'\n",
    "# ARCS_DBSCAN_FILTERED_DATA_DIR = '../data/eval_data/arcs_dbscan_filtered'\n",
    "ARCS_LABEL_FILTERED_DATA_DIR = '../data/eval_data/arcs_label_filtered'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f14b46",
   "metadata": {},
   "source": [
    "## Evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d48103d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 3D bbox object requires quaternion values. This function converts the yaw to these values\n",
    "def yaw_to_quaternion(yaw):\n",
    "    \"\"\"\n",
    "    Converts a yaw angle (rotation about the z-axis) to quaternion coordinates.\n",
    "    \n",
    "    Parameters:\n",
    "    - yaw (float): The yaw angle in radians.\n",
    "    \n",
    "    Returns:\n",
    "    - tuple of (rw, rx, ry, rz): The quaternion representation of the yaw.\n",
    "    \"\"\"\n",
    "    # Compute the components of the quaternion\n",
    "    rw = math.cos(yaw / 2.0)\n",
    "    rz = math.sin(yaw / 2.0)\n",
    "    \n",
    "    # rx and ry are zero because the rotation is only about the z-axis\n",
    "    return (rw, 0, 0, rz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "127142a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes x, y, z, dx, dy, dz, yaw and returns a 3D bbox object from the bbox package\n",
    "# Yaw is in radians\n",
    "def get3DBbox(x, y, z, dx, dy, dz, yaw):\n",
    "    # Example usage:\n",
    "    rw, rx, ry, rz = yaw_to_quaternion(yaw)\n",
    "    bbox_obj = bbox.BBox3D(x, y, z, length=dx, width=dy, height=dz, rw=rw, rx=rx, ry=ry, rz=rz)\n",
    "    return bbox_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c64892fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes two bounding boxes, returns the IoU\n",
    "def calculate_iou(bbox1, bbox2):\n",
    "    bbox_obj1 = get3DBbox(*bbox1)\n",
    "    bbox_obj2 = get3DBbox(*bbox2)\n",
    "    return bbox.metrics.jaccard_index_3d(bbox_obj1, bbox_obj2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "306ec6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_area = [21.0175, 11.717, -0.3925, 40.035, 55.9349, 7.535, 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "dac43616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns true if the pending box overlaps the evaluation area\n",
    "def is_in_eval_area(pending_bbox):\n",
    "    iou = calculate_iou(evaluation_area, pending_bbox)\n",
    "    if iou > 0:\n",
    "        return True\n",
    "    return False\n",
    "#     return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8ab78e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _evaluate frame takes a prediction dictionary output by the model, and a list of ground truths\n",
    "# from the label file, and returns the TPs, FPs, and FNs for one lidar frame\n",
    "def _evaluate_frame(predictions, ground_truths, iou_threshold=0.25):\n",
    "    TPs, FPs, FNs = 0, 0, len(ground_truths)\n",
    "    used_gt = set()\n",
    "    confidence_labels = []\n",
    "    \n",
    "    for pred in predictions['predictions']:\n",
    "        for label, score, bbox in zip(pred['labels_3d'], pred['scores_3d'], pred['bboxes_3d']):\n",
    "            # Only count the prediction if it's within the evaluation area\n",
    "            if is_in_eval_area(bbox):\n",
    "                best_iou = 0\n",
    "                best_gt = None\n",
    "                for gt in ground_truths:\n",
    "                    iou = calculate_iou(bbox, gt[8:])\n",
    "                    if iou > best_iou:\n",
    "                        best_iou = iou\n",
    "                        best_gt = tuple(gt)\n",
    "\n",
    "                if best_iou > iou_threshold:\n",
    "                    if best_gt not in used_gt:\n",
    "                        used_gt.add(best_gt)\n",
    "                        TPs += 1\n",
    "                        FNs -= 1\n",
    "                        confidence_labels.append((score, 1))\n",
    "                    else:\n",
    "                        FPs += 1\n",
    "                        confidence_labels.append((score, 0))\n",
    "                else:\n",
    "                    FPs += 1\n",
    "    return TPs, FPs, FNs, confidence_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "17acf892",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ground_truths(label_path):\n",
    "    labels = []\n",
    "    with open(label_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            bbox = []\n",
    "            # Add the category as a string\n",
    "            bbox.append(parts[0])\n",
    "            # Extract the bounding box dimensions and location as \n",
    "            bbox = bbox + [float(value) for value in parts[1:15]]  \n",
    "            bbox[2] = int(bbox[2])\n",
    "            # Only include the bbox if it's in the evaluation area\n",
    "            if is_in_eval_area(bbox[8:]):\n",
    "                labels.append(bbox)\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4d0c97d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_problem_file_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "2474084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# _evaluate frame takes a lidar and label file path and\n",
    "# and returns the TPs, FPs, and FNs for one lidar frame\n",
    "def evaluate_frame(lidar_file_path, label_file_path):\n",
    "    inputs = dict(points=str(lidar_file_path))\n",
    "    \n",
    "    # Get predictions\n",
    "    try:\n",
    "        predictions = inferencer(inputs)\n",
    "            # Get ground truths\n",
    "        ground_truths = parse_ground_truths(label_file_path)\n",
    "        num_ground_truths = len(ground_truths)\n",
    "\n",
    "        TPs, FPs, FNs, confidence_labels = _evaluate_frame(predictions, ground_truths)\n",
    "    except:\n",
    "        # Add file id to the problem file list\n",
    "        print(str(lidar_file_path))\n",
    "        lidar_filename = os.path.basename(lidar_file_path)\n",
    "        file_id, extension = os.path.splitext(lidar_filename)\n",
    "        generated_problem_file_list.append(file_id)\n",
    "        TPs, FPs, FNs, num_ground_truths, confidence_labels = 0, 0, 0, 0, []\n",
    "    \n",
    "    return TPs, FPs, FNs, num_ground_truths, confidence_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e35c8de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem files. I'll find out why later\n",
    "problem_file_ids = ['000522', '003066', '000556', '004340', '001016', '004224', '005318', '004344', '001014', '000522',\n",
    "                    '003066', '000556', '004340', '001016', '004224', '005318', '004344', '001014', '001027', '006440',\n",
    "                    '001026', '000869', '001031', '000530', '004928']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6bb44482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_summary(list_file_ids):\n",
    "    for id in list_file_ids:\n",
    "        print(f'\\n_____________________________________________________________')\n",
    "        \n",
    "        file_path = f'{ARCS_LABEL_FILTERED_DATA_DIR}/{id}.bin'\n",
    "        label_path = f'{ARCS_LABELS}/{id}.txt' \n",
    "\n",
    "        # Open point cloud file\n",
    "        points = np.fromfile(file_path, dtype=np.float32).reshape(-1, 4)\n",
    "        df_points = pd.DataFrame(points, columns=['x', 'y', 'z', 'intensity'])\n",
    "\n",
    "        # Print the DataFrame summary\n",
    "        print(f\"Summary for file ID {id}:\")\n",
    "        print('length: ' + str(len(df_points)))\n",
    "        print(\"DataFrame Info:\")\n",
    "        df_points.info()\n",
    "        print(\"\\nDataFrame Description:\")\n",
    "        print(df_points.describe())\n",
    "        print(f'\\nlabels:')\n",
    "        # Print labels\n",
    "        with open(label_path, 'r') as file:\n",
    "            for line in file:\n",
    "                print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afe0d5c",
   "metadata": {},
   "source": [
    "## Metrics calculation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "61392321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_precision_recall(predictions, num_ground_truths):\n",
    "    # Sort by confidence score in descending order\n",
    "    predictions.sort(key=lambda x: x[0], reverse=True)\n",
    "    \n",
    "    tp = 0\n",
    "    fp = 0\n",
    "    total_positives = sum(is_tp for _, is_tp in predictions)\n",
    "    \n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for conf, is_tp in predictions:\n",
    "        if is_tp:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "        \n",
    "        precision = tp / (tp + fp)\n",
    "        recall = tp / num_ground_truths\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    return precisions, recalls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7e6eb127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_ap(precisions, recalls):\n",
    "    # Calculate AP using the trapezoidal rule to compute the area under the curve\n",
    "    ap = 0\n",
    "    for i in range(1, len(recalls)):\n",
    "        ap += (recalls[i] - recalls[i-1]) * (precisions[i] + precisions[i-1]) / 2\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bab7d039",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_dataset(dataset_path, num_frames):\n",
    "    label_dir = Path(ARCS_LABELS)\n",
    "    dataset_dir = Path(dataset_path)\n",
    "    \n",
    "    total_TPs, total_FPs, total_FNs = 0, 0, 0\n",
    "    num_ground_truths = 0\n",
    "    confidence_labels = []\n",
    "\n",
    "    start = time.time()\n",
    "    for bin_file in itertools.islice(dataset_dir.iterdir(), num_frames):\n",
    "#     for bin_file in dataset_dir.iterdir():\n",
    "        if str(bin_file).endswith('.bin'):\n",
    "            print('.', end='')\n",
    "\n",
    "            lidar_filename = os.path.basename(bin_file)\n",
    "            # Split the filename from the extension ('006428', '.txt')\n",
    "            file_id, extension = os.path.splitext(lidar_filename)\n",
    "            if file_id not in problem_file_ids:\n",
    "                label_filename = file_id + '.txt'\n",
    "\n",
    "                # Make file paths\n",
    "                lidar_file_path = Path(dataset_dir, lidar_filename)\n",
    "                label_file_path = Path(label_dir, label_filename)\n",
    "                \n",
    "#                 print('evaluating: ' + str(lidar_file_path))\n",
    "                \n",
    "                TPs, FPs, FNs, num_frame_ground_truths, frame_confidence_labels = evaluate_frame(lidar_file_path, label_file_path)\n",
    "\n",
    "                total_TPs += TPs\n",
    "                total_FPs += FPs\n",
    "                total_FNs += FNs\n",
    "                num_ground_truths += num_frame_ground_truths\n",
    "                confidence_labels += frame_confidence_labels\n",
    "            else:\n",
    "                print('skipping ' + file_id)\n",
    "    end = time.time()\n",
    "        \n",
    "    return total_TPs, total_FPs, total_FNs, num_ground_truths, confidence_labels, (end - start) / num_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ea5180b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_pointpillars(dataset_path, dataset_name, num_frames=200):\n",
    "    print('Evaluating dataset: ' + dataset_path)\n",
    "    # Get all TP, FP, and FN in the dataset\n",
    "    TPs, FPs, FNs, num_ground_truths, confidence_labels, time_per_frame = evaluate_dataset(dataset_path, num_frames)\n",
    "    # Get metrics\n",
    "    precision = TPs / (TPs + FPs) if TPs + FPs > 0 else 0\n",
    "    recall = TPs / (TPs + FNs) if TPs + FNs > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    \n",
    "    # Placeholder for average precision calculation\n",
    "    precisions, recalls = calculate_precision_recall(confidence_labels, num_ground_truths)\n",
    "    average_precision = calculate_ap(precisions, recalls)\n",
    "\n",
    "    # Organize results into a dictionary\n",
    "    results = {\n",
    "        'Dataset': dataset_name,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1_score,\n",
    "        'Average Precision': average_precision,\n",
    "        'time_per_frame': time_per_frame\n",
    "    }\n",
    "    print(results)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "47f30b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run tests on ARCS\n",
    "results = test_pointpillars(ARCS_DATA_DIR, 'ARCS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b427d58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run tests on ARCS filtered\n",
    "azimuth_filter_results = test_pointpillars(ARCS_AZIMUTH_FILTERED_DATA_DIR, 'Azimuth Filtered ARCS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9881124e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run tests on ARCS filtered\n",
    "low_def_filter_results = test_pointpillars(ARCS_LOW_DEF_FILTERED_DATA_DIR, 'Low Def Filtered ARCS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4159272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Run tests on ARCS label filtered\n",
    "label_filter_results = test_pointpillars(ARCS_LABEL_FILTERED_DATA_DIR, 'Label Filtered ARCS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b703498d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(generated_problem_file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f030e4b0",
   "metadata": {},
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "cb79b3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(results)\n",
    "# print(filter_results)\n",
    "# print(label_filter_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a0dbad4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>Average Precision</th>\n",
       "      <th>time_per_frame</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ARCS</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.716814</td>\n",
       "      <td>0.645418</td>\n",
       "      <td>0.712389</td>\n",
       "      <td>0.146081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Low Def Filtered ARCS</td>\n",
       "      <td>0.275330</td>\n",
       "      <td>0.553097</td>\n",
       "      <td>0.367647</td>\n",
       "      <td>0.548673</td>\n",
       "      <td>0.150452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Azimuth Filtered ARCS</td>\n",
       "      <td>0.495298</td>\n",
       "      <td>0.699115</td>\n",
       "      <td>0.579817</td>\n",
       "      <td>0.694690</td>\n",
       "      <td>0.108186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Label Filtered ARCS</td>\n",
       "      <td>0.790909</td>\n",
       "      <td>0.769912</td>\n",
       "      <td>0.780269</td>\n",
       "      <td>0.765487</td>\n",
       "      <td>0.102220</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Dataset  Precision    Recall  F1 Score  Average Precision  \\\n",
       "0                   ARCS   0.586957  0.716814  0.645418           0.712389   \n",
       "1  Low Def Filtered ARCS   0.275330  0.553097  0.367647           0.548673   \n",
       "2  Azimuth Filtered ARCS   0.495298  0.699115  0.579817           0.694690   \n",
       "3    Label Filtered ARCS   0.790909  0.769912  0.780269           0.765487   \n",
       "\n",
       "   time_per_frame  \n",
       "0        0.146081  \n",
       "1        0.150452  \n",
       "2        0.108186  \n",
       "3        0.102220  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create DataFrame\n",
    "results_df = pd.DataFrame([results, low_def_filter_results, azimuth_filter_results, label_filter_results])\n",
    "display(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "572182fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Dataset  Precision   Recall  F1 Score  Average Precision  time_per_frame\n",
      "                 ARCS   0.586957 0.716814  0.645418           0.712389        0.146081\n",
      "Low Def Filtered ARCS   0.275330 0.553097  0.367647           0.548673        0.150452\n",
      "Azimuth Filtered ARCS   0.495298 0.699115  0.579817           0.694690        0.108186\n",
      "  Label Filtered ARCS   0.790909 0.769912  0.780269           0.765487        0.102220\n"
     ]
    }
   ],
   "source": [
    "print(results_df.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
