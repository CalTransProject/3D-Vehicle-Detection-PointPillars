{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6e7e623-3db1-47b0-a2a9-dd076b04be2b",
   "metadata": {},
   "source": [
    "# Filter each stationary frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7a689bb4-b129-40f6-a07a-2c94e6604124",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import itertools\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0bd788e-d8f5-4621-84a5-764b6f1f59ff",
   "metadata": {},
   "source": [
    "## Constants and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f77dba0c-506d-41f7-94e0-1f78bd80b165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set background map resolutions\n",
    "azimuth_resolution = 1\n",
    "height_resolution = 0.5\n",
    "std_dev_cutoff = 1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73db7eb6-b3f9-4a18-b7bd-a7d6d7515511",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR_ROOT = '../data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b776a837-da24-497c-ae95-997425d96683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to true if you want to make a new map, otherwise, will load previous map\n",
    "make_new_distances_df = False\n",
    "make_new_lookup_table = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9373699a-6432-447a-9181-6f778fc03001",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "af74dafa-7133-428b-8799-dbe0d7d67127",
   "metadata": {},
   "source": [
    "## First, functions for making background map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2b0b2769-48b9-4b86-a127-a6ac5089a830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mappings for azimuth and height using integers\n",
    "def create_mappings(azimuth_step=azimuth_resolution, height_step=height_resolution):\n",
    "    azimuth_range = np.arange(-180, 180 + azimuth_step, azimuth_step)\n",
    "    height_range = np.arange(-30, 10 + height_step, height_step)\n",
    "    azimuth_map = {int(az * 10): idx for idx, az in enumerate(azimuth_range)}\n",
    "    height_map = {int(ht * 10): idx for idx, ht in enumerate(height_range)}\n",
    "    return azimuth_map, height_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39f724e2-15bf-4f35-b560-9d0d15606462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the grid DataFrames\n",
    "def create_grid_dataframes():\n",
    "    azimuth_map, height_map = create_mappings()\n",
    "    grid_shape = (len(height_map), len(azimuth_map))\n",
    "    df_distances = pd.DataFrame({key: [[] for _ in range(len(height_map))] \\\n",
    "                                 for key in azimuth_map.keys()}, index=height_map.keys())\n",
    "    df_intensities = pd.DataFrame({key: [[] for _ in range(len(height_map))] \\\n",
    "                                   for key in azimuth_map.keys()}, index=height_map.keys())\n",
    "    return df_distances, df_intensities, azimuth_map, height_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "87791ed8-709c-43ec-808f-a7f735d6cf9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process file into grid\n",
    "def process_files_to_grid(data_dir):\n",
    "    # Create empty grid\n",
    "    df_distances, df_intensities, azimuth_map, height_map = create_grid_dataframes()\n",
    "\n",
    "    lidar_dir = Path(data_dir, 'velodyne_points')\n",
    "    # For each file in the directory\n",
    "    for file_path in lidar_dir.iterdir():\n",
    "    # for file_path in itertools.islice(lidar_dir.iterdir(), 400):\n",
    "        print('-', end ='')\n",
    "        data = np.fromfile(file_path, dtype=np.float32).reshape(-1, 4)\n",
    "        for x, y, z, intensity in data:\n",
    "            # Convert to azimuth, height, distance format\n",
    "            distance = np.sqrt(x**2 + y**2 + z**2)\n",
    "            azimuth = np.degrees(np.arctan2(y, x))\n",
    "            height = np.degrees(np.arctan2(z, np.sqrt(x**2 + y**2)))\n",
    "            # Convert and scale\n",
    "            azimuth_idx = int(np.floor((azimuth + 180) / \\\n",
    "                                       azimuth_resolution) * azimuth_resolution * 10) - 1800\n",
    "            height_idx = int(np.floor((height + 30) / height_resolution)) - 300\n",
    "            # Update DataFrames directly using indices\n",
    "            if azimuth_idx in azimuth_map and height_idx in height_map:\n",
    "                df_distances.at[height_idx, azimuth_idx].append(distance)\n",
    "                # df_intensities.at[height_idx, azimuth_idx].append(intensity)\n",
    "    # return df_distances, df_intensities\n",
    "    return df_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a3047f-0ff9-48d2-b55d-32db785fa222",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get distances_dataframe(dir):\n",
    "    # Get the distances of the background map from the lidar files\n",
    "    # if make_new_map or not files_exist(\"df_distances.pkl\", \"df_intensities.pkl\"):\n",
    "    if make_new_distances_df:\n",
    "        df_distances = process_files_to_grid(dir)\n",
    "        \n",
    "        # Save the DataFrames\n",
    "        print('\\nSaving dataframe')\n",
    "        df_distances.to_pickle(\"new_df_distances.pkl\")\n",
    "    else:\n",
    "        # Load the DataFrames\n",
    "        print('\\nLoading dataframe')\n",
    "        df_distances = pd.read_pickle(\"df_distances.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24d4269d-47cb-493e-b4ca-5b2d38c6f39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def files_exist(*files):\n",
    "    return all(os.path.exists(file) for file in files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "e27c9a29-1f32-447a-b08f-0dcf82274fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def create_background_lookup_table(df_distances):\n",
    "    if make_new_lookup_table:\n",
    "        df_distances = process_files_to_grid(dir)\n",
    "        \n",
    "        # Save the DataFrames\n",
    "        print('\\nSaving lookup table')\n",
    "        df_distances.to_pickle(\"new_lookup_table.pkl\")\n",
    "    else:\n",
    "        # Load the DataFrames\n",
    "        print('\\nLoading lookup table')\n",
    "        df_distances = pd.read_pickle(\"dbscan_lookup_table.pkl\")\n",
    "    # Create a new DataFrame with the same index and columns as df_distances\n",
    "    lookup_table = pd.DataFrame(index=df_distances.index, columns=df_distances.columns)\n",
    "    # Iterate through each cell in df_distances\n",
    "    for (height, azimuth), distances in df_distances.stack().items():\n",
    "        if distances:  # If the list is not empty\n",
    "            \n",
    "            distances = np.array(distances).reshape(-1, 1)\n",
    "\n",
    "            # Note: Adjust eps and min_samples based on the distribution of your data\n",
    "            dbscan = DBSCAN(eps=10, min_samples=2).fit(distances)\n",
    "            \n",
    "            # Step 2: Identify the cluster labels\n",
    "            labels = dbscan.labels_\n",
    "            \n",
    "            # Step 3: Filter out noise and find the cluster with the largest numbers\n",
    "            # Find indices of the cluster with the maximum mean (or maximum minimum to find the farthest cluster)\n",
    "            clusters = {}\n",
    "            for label in np.unique(labels):\n",
    "                if label != -1:  # Ignore noise if present\n",
    "                    cluster_members = distances[labels == label].flatten()\n",
    "                    clusters[label] = cluster_members\n",
    "            \n",
    "            # Step 4: Find the cluster with the largest numbers (farthest distances)\n",
    "            # Using max of minimums of each cluster to ensure we're finding the cluster farthest away\n",
    "            farthest_cluster_label = max(clusters, key=lambda x: clusters[x].min())\n",
    "            \n",
    "            # Step 5: Find the smallest number in the farthest cluster\n",
    "            smallest_in_farthest = clusters[farthest_cluster_label].min()\n",
    "\n",
    "\n",
    "            \n",
    "            # sorted_distances = np.sort(distances)\n",
    "\n",
    "            # # Drop the nearest half\n",
    "            # # remaining_distances = sorted_distances[len(sorted_distances) // 2:]\n",
    "            # if len(distances) >= 10:\n",
    "            #     remaining_distances = sorted_distances[-10:]\n",
    "            # else:\n",
    "            #     remaining_distances = sorted_distances\n",
    "           \n",
    "            # # Calculate the value as the largest distance minus the standard deviation\n",
    "            # # value = np.max(distances) - (2 * np.std(distances))\n",
    "            # cutoff = np.median(remaining_distances)\n",
    "            # standard_deviation = np.std(remaining_distances)\n",
    "            # adjustment = standard_deviation\n",
    "            \n",
    "            # cutoff = cutoff - adjustment\n",
    "                \n",
    "            value = smallest_in_farthest\n",
    "        else:\n",
    "            value = np.nan  # If the list is empty, set the cell to NaN\n",
    "\n",
    "        # Set the value in the new DataFrame\n",
    "        lookup_table.at[height, azimuth] = value\n",
    "\n",
    "    return lookup_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6231c973-5d7e-476e-9fc5-44c09d4ef27c",
   "metadata": {},
   "source": [
    "## Filtering and saving functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "af930e13-e6a1-43ce-86e0-666e9f94801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_dataframe(bin_path):\n",
    "    pre_filtered_data = np.fromfile(bin_path, dtype=np.float32).reshape(-1, 4) \n",
    "    columns = ['x', 'y', 'z', 'intensity']\n",
    "    df = pd.DataFrame(pre_filtered_data, columns=columns)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b95c4d38-7e51-4d13-8b98-8c683e7ba7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_lookup_coords_to_xyz(points_df):\n",
    "    # Calculate the distance, azimuth, and height using vectorized operations\n",
    "    x, y, z, intensity = points_df['x'], points_df['y'], points_df['z'], points_df['intensity']\n",
    "    distance = np.sqrt(x**2 + y**2 + z**2)\n",
    "    azimuth = np.degrees(np.arctan2(y, x))\n",
    "    height = np.degrees(np.arctan2(z, np.sqrt(x**2 + y**2)))\n",
    "    \n",
    "    # Convert and scale\n",
    "    azimuth_idx = np.floor((azimuth + 180) / azimuth_resolution).astype(int) \\\n",
    "        * azimuth_resolution * 10 - 1800\n",
    "    height_idx = np.floor((height + 30) / height_resolution).astype(int) - 300\n",
    "    \n",
    "    # Add new columns to dataframe\n",
    "    points_df['distance'] = distance\n",
    "    points_df['azimuth_idx'] = azimuth_idx\n",
    "    points_df['height_idx'] = height_idx\n",
    "    \n",
    "    return points_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "488d3c76-e60d-4011-9b58-ce05ec7aded6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_points(input_file, lookup_table):\n",
    "    # Get dataframe from file\n",
    "    pre_filtered_points = convert_to_dataframe(input_file)\n",
    "\n",
    "    # Add lookup table coordinates\n",
    "    pre_filtered_grid_lookup = add_lookup_coords_to_xyz(pre_filtered_points)\n",
    "    \n",
    "    # Initialize a list to store rows that meet the criteria\n",
    "    filtered_data = []\n",
    "\n",
    "    # Iterate through each row in the input DataFrame\n",
    "    # for idx, row in pre_filtered_grid_lookup.iloc[:10].iterrows():\n",
    "    for idx, row in pre_filtered_grid_lookup.iterrows():\n",
    "        azimuth_idx = int(row['azimuth_idx'])\n",
    "        height_idx = int(row['height_idx'])\n",
    "        \n",
    "        # Check if the indices exist in the lookup table and the value is not NaN\n",
    "        if azimuth_idx in lookup_table.columns and height_idx in lookup_table.index:\n",
    "            # print('.', end='')\n",
    "            lookup_value = lookup_table.at[height_idx, azimuth_idx]\n",
    "            \n",
    "            if not pd.isna(lookup_value) and row['distance'] < lookup_value:\n",
    "                # If criteria are met, add the row's x, y, z, and intensity to the filtered_data list\n",
    "                filtered_data.append({\n",
    "                    'x': row['x'],\n",
    "                    'y': row['y'],\n",
    "                    'z': row['z'],\n",
    "                    'intensity': row['intensity']\n",
    "                })\n",
    "\n",
    "    # Create a DataFrame from the filtered data\n",
    "    filtered_df = pd.DataFrame(filtered_data)\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "e77a6403-4e34-45d3-8ae4-f3ca96710410",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_as_binary(df, bin_path):\n",
    "    # Ensure the DataFrame is in the correct order and data type\n",
    "    data = df[['x', 'y', 'z', 'intensity']].astype(np.float32).values\n",
    "    \n",
    "    # Write the data to a binary file\n",
    "    data.tofile(bin_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9f742807-8599-44f8-a52e-7b5b14f12cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_frames(dir, background_lookup_table):\n",
    "    # Create a new folder for the filtered frames in the directory\n",
    "    new_save_location = Path(dir, 'filtered_points')\n",
    "    new_save_location.mkdir(exist_ok=True)\n",
    "    \n",
    "    lidar_dir = Path(dir, 'velodyne_points')\n",
    "    \n",
    "    # Get just the file names\n",
    "    files = [f for f in os.listdir(lidar_dir) if f.endswith('.bin')]\n",
    "    # For each file\n",
    "    for filename in files[:30]:\n",
    "    # for filename in files:\n",
    "        # Append file name to location\n",
    "        print('.', end='')\n",
    "        from_file = Path(lidar_dir, filename)\n",
    "\n",
    "        # Filter file\n",
    "        filtered_df = filter_points(from_file, background_lookup_table)\n",
    "\n",
    "        # APPEND FILE NAME TO NEW LOCATION\n",
    "        to_file = Path(new_save_location, filename)\n",
    "\n",
    "        # CONVERT BACK TO BINARY and save\n",
    "        save_as_binary(filtered_df, to_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "639cb573-5c95-4d31-aa22-862a1b2a4077",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "bad allocation",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[69], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m map_save_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest_map\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Create background lookup table for distance cutoffs\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m background_distance_lookup_table \u001b[38;5;241m=\u001b[39m create_background_lookup_table(\u001b[38;5;28mdir\u001b[39m)\n",
      "Cell \u001b[1;32mIn[67], line 25\u001b[0m, in \u001b[0;36mcreate_background_lookup_table\u001b[1;34m(dir)\u001b[0m\n\u001b[0;32m     22\u001b[0m distances \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(distances)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Note: Adjust eps and min_samples based on the distribution of your data\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m dbscan \u001b[38;5;241m=\u001b[39m DBSCAN(eps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, min_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\u001b[38;5;241m.\u001b[39mfit(distances)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Step 2: Identify the cluster labels\u001b[39;00m\n\u001b[0;32m     28\u001b[0m labels \u001b[38;5;241m=\u001b[39m dbscan\u001b[38;5;241m.\u001b[39mlabels_\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_dbscan.py:436\u001b[0m, in \u001b[0;36mDBSCAN.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# A list of all core samples found.\u001b[39;00m\n\u001b[0;32m    435\u001b[0m core_samples \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(n_neighbors \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_samples, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39muint8)\n\u001b[1;32m--> 436\u001b[0m dbscan_inner(core_samples, neighborhoods, labels)\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcore_sample_indices_ \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(core_samples)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels_ \u001b[38;5;241m=\u001b[39m labels\n",
      "File \u001b[1;32m_dbscan_inner.pyx:33\u001b[0m, in \u001b[0;36msklearn.cluster._dbscan_inner.dbscan_inner\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: bad allocation"
     ]
    }
   ],
   "source": [
    "dir = Path(DATA_DIR_ROOT)\n",
    "map_save_name = 'test_map'\n",
    "\n",
    "get\n",
    "# Create background lookup table for distance cutoffs\n",
    "background_distance_lookup_table = create_background_lookup_table(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f7d44ac6-1229-4c91-862c-18a44cfccd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".............................."
     ]
    }
   ],
   "source": [
    "# Filter and save each filtered frame\n",
    "filter_frames(dir, background_distance_lookup_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7d57fb06-5170-45af-b9f3-4fcb27ee9ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "background_distance_lookup_table.to_pickle(\"dbscan_lookup_table.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
